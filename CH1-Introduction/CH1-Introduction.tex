\documentclass{article}

\usepackage{amsmath}
\usepackage{numprint}

\author{yuyang339@gmail.com}
\title{Pattern Recognition and Machine learning}
\date{}
\begin{document}

\maketitle

\textbf{Disclaimer.} Use of the information contained within this project is at your sole risk. The project maintainer doesn't guarentee the solutions are correct.

\section*{Chapter 1 Introduction}
\subsection*{Problem 1}
\begin{flushleft}
Apply partial derivative to $E(\boldsymbol{\omega})$ with respect to $\omega_{i}$ and set it to zero.

\begin{equation}
\frac{\partial E(\boldsymbol{\omega})}{\partial \omega_{i}} = \sum_{n=1}^{N}{(y-t_{n})}\frac{\partial y}{\partial \omega_{i}} = \sum_{n=1}^{N}{(y-t_{n})}(x_{n})^i =\sum_{n=1}^{N}{(\sum_{j=0}^{M}\omega_{j}(x_{n})^{j}-t_{n})}(x_{n})^i = 0
\end{equation}

\begin{equation}
\sum_{j=0}^{M}\omega_{j}\sum_{n=1}^{N}{(x_{n})^{i+j}= \sum_{n=1}^{N}t_{n}}(x_{n})^i 
\end{equation}


\subsection*{Problem 2}
Similar to Problem 1
\begin{equation}
\frac{\partial E(\boldsymbol{\omega})}{\partial \omega_{i}} = \sum_{n=1}^{N}{(y-t_{n})}\frac{\partial y}{\partial \omega_{i}} + 2\lambda\omega_{i} = \sum_{n=1}^{N}{(y-t_{n})}(x_{n})^i + 2\omega_{i} =\sum_{n=1}^{N}{(\sum_{j=0}^{M}\omega_{j}(x_{n})^{j}-t_{n})}(x_{n})^i + 2\lambda\omega_{i}= 0
\end{equation}

\begin{equation}
\sum_{j=0}^{M}\omega_{j}\sum_{n=1}^{N}{(x_{n})^{i+j} + 2\lambda\omega_{i}= \sum_{n=1}^{N}t_{n}}(x_{n})^i 
\end{equation}

\subsection*{Problem 3}
(1) the probability of selecting an apple is
\begin{equation}
\begin{split}
P(Orange) = P(Orange|Green Box)P(Green Box) + \\
 P(Orange|Red Box)P(Red Box)+ \\
 P(Orange|Blue Box)P(Blue Box) = 0.34
 \end{split}
\end{equation}
(2) the probability that the seletd fruit came from the green box is 

\begin{equation}
P(Green box|Orange) = \frac{P(Orange|Green Box)}{P(Orange)} = \frac{0.18}{0.34}
\end{equation}

\subsection*{Problem 3}
Under a nonlinear change of variable, a probability density transforms differently from a simple function, due to the Jacobian factor.

Now consider the behaviour of a probability density $p_x(x)$ under the change of variables $x = g(y)$, where the density with respect to the new variable is $p_y(y)$ and is given by (1.27).

Let us write $\lvert{g^{'}(y)}\rvert = sg^{'}(y)$ where 
$s\in{âˆ’1, +1}$. Then ((1.27)) can be written
\begin{equation}
p_y(y) = p_x(g(y))sg^{'}(y)
\end{equation}
Differentiating both sides with respect to y then gives
\begin{equation}
p_y^{'}(y) = sp_x^{'}(g(y)){g^{'}(y)}^2 + sp_x(g(y))g^{''}(y)
\end{equation}






Let's discuss it in two cases.
The first case is the transformation of $x=g(y)$ is non-linear.
Since the second term of the right-hand side of equation (8) is not zero, the value of x obtained by maximizing $p_x(x)$ will not be the value obtained by transforming to $p_y(y)$ then maximizing with respect to $y$ and then transforming back to $x$. 

In the case that the transformation of $x=g(y)$ is non-linear, the  the second term of the right-hand side of equation (8) is not zero, so the value of x obtained by maximizing $p_x(x)$ will be the value obtained by transforming to $p_y(y)$ then maximizing with respect to $y$ and then transforming back to $x$.
\end{flushleft}


\end{document}
